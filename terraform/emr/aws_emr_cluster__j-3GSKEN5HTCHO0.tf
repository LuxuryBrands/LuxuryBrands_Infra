#Â File generated by aws2tf see https://github.com/aws-samples/aws2tf
# aws_emr_cluster.j-3GSKEN5HTCHO0:
resource "aws_emr_cluster" "j-3GSKEN5HTCHO0" {
lifecycle {
ignore_changes = [kerberos_attributes[0].kdc_admin_password]
}
    applications           = [
        "Spark",
        "Zeppelin",
    ]
    ebs_root_volume_size   = 0
    log_uri                = "s3n://de-2-1-s3/emr/"
    name                   = "de-2-1-emr"
    placement_group_config = []
    release_label          = "emr-6.12.0"
    scale_down_behavior    = "TERMINATE_AT_TASK_COMPLETION"
service_role = aws_iam_role.r-arn:aws:iam::862327261051:role/EMR_DefaultRole.name
    step                   = [
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_18",
                        "2023-09-01T18:38:31.428424+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "cluster",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_18",
                        "2023-09-01T18:47:27.385414+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "--conf",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_18",
                        "2023-09-01T18:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_18",
                        "2023-09-01T18:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_19",
                        "2023-09-01T19:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_20",
                        "2023-09-01T20:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_21",
                        "2023-09-01T21:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_22",
                        "2023-09-01T22:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-01/brand_2023-09-01_23",
                        "2023-09-01T23:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_00",
                        "2023-09-02T00:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_00",
                        "2023-09-02T00:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_01",
                        "2023-09-02T01:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_01",
                        "2023-09-02T01:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_02",
                        "2023-09-02T02:07:51.566707+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "cluster",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_02",
                        "2023-09-02T02:20:03.786422+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_02",
                        "2023-09-02T02:23:20.332510+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_02",
                        "2023-09-02T02:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_02",
                        "2023-09-02T02:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_03",
                        "2023-09-02T03:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_03",
                        "2023-09-02T03:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_04",
                        "2023-09-02T04:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_04",
                        "2023-09-02T04:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_05",
                        "2023-09-02T05:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_06",
                        "2023-09-02T06:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://$de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_07",
                        "2023-09-02T07:30:33.841166+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://$de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_07",
                        "2023-09-02T07:57:59.571388+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://$de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_07",
                        "2023-09-02T07:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_07",
                        "2023-09-02T07:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://$de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_08",
                        "2023-09-02T08:11:28.091607+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_08",
                        "2023-09-02T08:11:28.091607+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_08",
                        "2023-09-02T08:11:28.091607+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_08",
                        "2023-09-02T08:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_08",
                        "2023-09-02T08:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_08",
                        "2023-09-02T08:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_08",
                        "2023-09-02T08:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_09",
                        "2023-09-02T09:43:37.381226+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_08",
                        "2023-09-02T08:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_09",
                        "2023-09-02T09:43:37.381226+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_09",
                        "2023-09-02T09:00:00+00:00",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_09",
                        "2023-09-02T09:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_09",
                        "2023-09-02T09:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_09",
                        "2023-09-02T09:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_10",
                        "2023-09-02T10:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_10",
                        "2023-09-02T10:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_10",
                        "2023-09-02T10:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_10",
                        "2023-09-02T10:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_10",
                        "2023-09-02T10:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_10",
                        "2023-09-02T10:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_09",
                        "2023-09-02T09:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_11",
                        "2023-09-02T11:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_11",
                        "2023-09-02T11:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_11",
                        "2023-09-02T11:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_11",
                        "2023-09-02T11:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_11",
                        "2023-09-02T11:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_11",
                        "2023-09-02T11:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_12",
                        "2023-09-02T12:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_12",
                        "2023-09-02T12:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_12",
                        "2023-09-02T12:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_13",
                        "2023-09-02T13:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_13",
                        "2023-09-02T13:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_13",
                        "2023-09-02T13:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-02/media_2023-09-02_14",
                        "2023-09-02T14:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-02/brand_2023-09-02_14",
                        "2023-09-02T14:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-02/media_hashtag_2023-09-02_14",
                        "2023-09-02T14:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_06",
                        "2023-09-03T06:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_06",
                        "2023-09-03T06:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_06",
                        "2023-09-03T06:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_07",
                        "2023-09-03T07:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_07",
                        "2023-09-03T07:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_07",
                        "2023-09-03T07:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_08",
                        "2023-09-03T08:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_08",
                        "2023-09-03T08:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_08",
                        "2023-09-03T08:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_09",
                        "2023-09-03T09:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_09",
                        "2023-09-03T09:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_09",
                        "2023-09-03T09:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_10",
                        "2023-09-03T10:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_10",
                        "2023-09-03T10:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_10",
                        "2023-09-03T10:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_11",
                        "2023-09-03T11:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_11",
                        "2023-09-03T11:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_11",
                        "2023-09-03T11:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_12",
                        "2023-09-03T12:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_12",
                        "2023-09-03T12:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_12",
                        "2023-09-03T12:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_13",
                        "2023-09-03T13:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_13",
                        "2023-09-03T13:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_13",
                        "2023-09-03T13:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_14",
                        "2023-09-03T14:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_14",
                        "2023-09-03T14:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_14",
                        "2023-09-03T14:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_15",
                        "2023-09-03T15:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_15",
                        "2023-09-03T15:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_15",
                        "2023-09-03T15:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_16",
                        "2023-09-03T16:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_16",
                        "2023-09-03T16:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_16",
                        "2023-09-03T16:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_17",
                        "2023-09-03T17:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_17",
                        "2023-09-03T17:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_17",
                        "2023-09-03T17:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_18",
                        "2023-09-03T18:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_18",
                        "2023-09-03T18:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_18",
                        "2023-09-03T18:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_19",
                        "2023-09-03T19:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_19",
                        "2023-09-03T19:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_19",
                        "2023-09-03T19:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_20",
                        "2023-09-03T20:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_20",
                        "2023-09-03T20:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_20",
                        "2023-09-03T20:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_21",
                        "2023-09-03T21:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_21",
                        "2023-09-03T21:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_21",
                        "2023-09-03T21:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_22",
                        "2023-09-03T22:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_22",
                        "2023-09-03T22:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_22",
                        "2023-09-03T22:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-03/brand_2023-09-03_23",
                        "2023-09-03T23:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-03/media_2023-09-03_23",
                        "2023-09-03T23:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-03/media_hashtag_2023-09-03_23",
                        "2023-09-03T23:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_00",
                        "2023-09-04T00:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_00",
                        "2023-09-04T00:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_00",
                        "2023-09-04T00:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_01",
                        "2023-09-04T01:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_01",
                        "2023-09-04T01:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_01",
                        "2023-09-04T01:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_02",
                        "2023-09-04T02:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_02",
                        "2023-09-04T02:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_02",
                        "2023-09-04T02:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_03",
                        "2023-09-04T03:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_04",
                        "2023-09-04T04:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_05",
                        "2023-09-04T05:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_05",
                        "2023-09-04T05:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_05",
                        "2023-09-04T05:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_06",
                        "2023-09-04T06:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_06",
                        "2023-09-04T06:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_06",
                        "2023-09-04T06:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_07",
                        "2023-09-04T07:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_07",
                        "2023-09-04T07:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_07",
                        "2023-09-04T07:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_08",
                        "2023-09-04T08:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_08",
                        "2023-09-04T08:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_08",
                        "2023-09-04T08:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_09",
                        "2023-09-04T09:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_09",
                        "2023-09-04T09:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_09",
                        "2023-09-04T09:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media.py",
                        "s3a://de-2-1-s3/stage/media_2023-09-04/media_2023-09-04_10",
                        "2023-09-04T10:00:00+00:00",
                        "media",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_brand.py",
                        "s3a://de-2-1-s3/stage/brand_2023-09-04/brand_2023-09-04_10",
                        "2023-09-04T10:00:00+00:00",
                        "brand",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_brand_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "--packages",
                        "com.amazon.deequ:deequ:2.0.3-spark-3.3",
                        "--exclude-packages",
                        "net.sourceforge.f2j:arpack_combined_all",
                        "--py-files",
                        "s3://de-2-1-s3/emr/scripts/etl_job.py",
                        "s3://de-2-1-s3/emr/scripts/spark_media_hashtag.py",
                        "s3a://de-2-1-s3/stage/media_hashtag_2023-09-04/media_hashtag_2023-09-04_10",
                        "2023-09-04T10:00:00+00:00",
                        "media_hashtag",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "etl_media_hashtag_data"
        },
        {
            action_on_failure = "CANCEL_AND_WAIT"
            hadoop_jar_step   = [
                {
                    args       = [
                        "spark-submit",
                        "--deploy-mode",
                        "client",
                        "s3://de-2-1-s3/emr/scripts/emr_elt_redshift.py",
                    ]
                    jar        = "command-runner.jar"
                    main_class = ""
                    properties = {}
                },
            ]
            name              = "elt_data"
        },
    ]
    step_concurrency_level = 1
    tags                   = {
        "Team" = "DE-2-1"
    }
    tags_all               = {
        "Team" = "DE-2-1"
    }
    termination_protection = false
    visible_to_all_users   = true

    auto_termination_policy {
        idle_timeout = 21600
    }

    bootstrap_action {
        args = []
        name = "Bootstrap action"
        path = "s3://de-2-1-s3/config/emr_bootstrap.sh"
    }

    core_instance_group {
        instance_count = 1
        instance_type  = "m4.large"
        name           = "ì½ì´"

        ebs_config {
            iops                 = 0
            size                 = 32
            throughput           = 0
            type                 = "gp2"
            volumes_per_instance = 1
        }
    }

    ec2_attributes {
emr_managed_master_security_group = aws_security_group.sg-07d5717ff00eb8f3f.id
emr_managed_slave_security_group = aws_security_group.sg-00f8aed2039c9e343.id
instance_profile = aws_iam_instance_profile.EMR_EC2_DefaultRole.name
        key_name                          = "DE-2-1-key"
subnet_id = aws_subnet.subnet-03a832fb5b39f193e.id
    }

    master_instance_group {
        instance_count = 1
        instance_type  = "m4.large"
        name           = "íë¼ì´ë¨¸ë¦¬"

        ebs_config {
            iops                 = 0
            size                 = 32
            throughput           = 0
            type                 = "gp2"
            volumes_per_instance = 1
        }
    }
}
